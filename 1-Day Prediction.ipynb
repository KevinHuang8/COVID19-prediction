{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T04:05:29.547120Z",
     "start_time": "2020-04-25T04:05:29.538839Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jay/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:20: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "from numpy import concatenate\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import numpy as np\n",
    "\n",
    "from pandas import DataFrame\n",
    "from pandas import Series\n",
    "from pandas import concat\n",
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "from numpy import array\n",
    "\n",
    "\n",
    "import multiprocessing as mp\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T04:05:32.571019Z",
     "start_time": "2020-04-25T04:05:32.567129Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/device:CPU:0', '/device:XLA_CPU:0', '/device:XLA_GPU:0', '/device:GPU:0']\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as tfback    \n",
    "devices = tf.config.list_logical_devices()\n",
    "\n",
    "tfback._LOCAL_DEVICES = [x.name for x in devices]\n",
    "print(tfback._LOCAL_DEVICES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T02:49:47.716220Z",
     "start_time": "2020-04-25T02:49:47.690807Z"
    },
    "code_folding": [
     1
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# convert time series into supervised learning problem\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    "\n",
    "# create a differenced series\n",
    "\n",
    "\n",
    "def difference(dataset, interval=1):\n",
    "    diff = list()\n",
    "    for i in range(interval, len(dataset)):\n",
    "        value = dataset[i] - dataset[i - interval]\n",
    "        diff.append(value)\n",
    "    return Series(diff)\n",
    "\n",
    "# transform series into train and test sets for supervised learning\n",
    "\n",
    "\n",
    "def prepare_data(series, n_test, n_lag, n_seq):\n",
    "    # extract raw values\n",
    "    raw_values = series.values\n",
    "    # transform data to be stationary\n",
    "    diff_series = difference(raw_values, 1)\n",
    "    diff_values = diff_series.values\n",
    "    diff_values = diff_values.reshape(len(diff_values), 1)\n",
    "    # rescale values to -1, 1\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaled_values = scaler.fit_transform(diff_values)\n",
    "    scaled_values = scaled_values.reshape(len(scaled_values), 1)\n",
    "    # transform into supervised learning problem X, y\n",
    "    supervised = series_to_supervised(scaled_values, n_lag, n_seq)\n",
    "    supervised_values = supervised.values\n",
    "    # split into train and test sets\n",
    "    train, test = supervised_values[0:-n_test], supervised_values[-n_test:]\n",
    "    return scaler, train, test\n",
    "\n",
    "# invert differenced forecast\n",
    "\n",
    "\n",
    "def inverse_difference(last_ob, forecast):\n",
    "    # invert first forecast\n",
    "    inverted = list()\n",
    "    inverted.append(forecast[0] + last_ob)\n",
    "    # propagate difference forecast using inverted first value\n",
    "    for i in range(1, len(forecast)):\n",
    "        inverted.append(forecast[i] + inverted[i-1])\n",
    "    return inverted\n",
    "\n",
    "# inverse data transform on forecasts\n",
    "\n",
    "\n",
    "def inverse_transform(series, forecasts, scaler, n_test):\n",
    "    inverted = list()\n",
    "    for i in range(len(forecasts)):\n",
    "        # create array from forecast\n",
    "        forecast = array(forecasts[i])\n",
    "        forecast = forecast.reshape(1, len(forecast))\n",
    "        # invert scaling\n",
    "        inv_scale = scaler.inverse_transform(forecast)\n",
    "        inv_scale = inv_scale[0, :]\n",
    "        # invert differencing\n",
    "        index = len(series) - n_test + i - 1\n",
    "        last_ob = series.values[index]\n",
    "        inv_diff = inverse_difference(last_ob, inv_scale)\n",
    "        # store\n",
    "        inverted.append(inv_diff)\n",
    "    return inverted\n",
    "\n",
    "# fit an LSTM network to training data\n",
    "\n",
    "\n",
    "def fit_lstm(train, n_lag, n_seq, n_batch, nb_epoch, n_neurons):\n",
    "    # reshape training into [samples, timesteps, features]\n",
    "    g = tf.compat.v1.get_default_graph()\n",
    "    g.finalize()\n",
    "    X, y = train[:, 0:n_lag], train[:, n_lag:]\n",
    "    X = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "    # design network\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(n_neurons, batch_input_shape=(\n",
    "         n_batch, X.shape[1], X.shape[2]), stateful=True))\n",
    "    model.add(Dense(y.shape[1]))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    # fit network\n",
    "    for i in range(nb_epoch):\n",
    "        model.fit(X, y, epochs=1, batch_size=n_batch,\n",
    "                  verbose=0, shuffle=False)\n",
    "        model.reset_states()\n",
    "    return model\n",
    "\n",
    "# make one forecast with an LSTM,\n",
    "\n",
    "\n",
    "def forecast_lstm(model, X, n_batch):\n",
    "        # reshape input pattern to [samples, timesteps, features]\n",
    "    X = X.reshape(1, 1, len(X))\n",
    "    # make forecast\n",
    "    forecast = model.predict(X, batch_size=n_batch)\n",
    "    # convert to array\n",
    "    return [x for x in forecast[0, :]]\n",
    "\n",
    "# evaluate the persistence model\n",
    "\n",
    "\n",
    "def make_forecasts(model, n_batch, train, test, n_lag, n_seq):\n",
    "    forecasts = list()\n",
    "    for i in range(len(test)):\n",
    "        X, y = test[i, 0:n_lag], test[i, n_lag:]\n",
    "        # make forecast\n",
    "        forecast = forecast_lstm(model, X, n_batch)\n",
    "        # store the forecast\n",
    "        forecasts.append(forecast)\n",
    "    return forecasts\n",
    "\n",
    "\n",
    "# evaluate the RMSE for each forecast time step\n",
    "def evaluate_forecasts(test, forecasts, n_lag, n_seq):\n",
    "    for i in range(n_seq):\n",
    "        actual = [row[i] for row in test]\n",
    "        predicted = [forecast[i] for forecast in forecasts]\n",
    "        rmse = sqrt(mean_squared_error(actual, predicted))\n",
    "        print('t+%d RMSE: %f' % ((i+1), rmse))\n",
    "\n",
    "# plot the forecasts in the context of the original dataset\n",
    "\n",
    "\n",
    "def plot_forecasts(series, forecasts, n_test):\n",
    "    # plot the entire dataset in blue\n",
    "    pyplot.plot(series.values)\n",
    "    # plot the forecasts in red\n",
    "    for i in range(len(forecasts)):\n",
    "        off_s = len(series) - n_test + i - 1\n",
    "        off_e = off_s + len(forecasts[i]) + 1\n",
    "        xaxis = [x for x in range(off_s, off_e)]\n",
    "        yaxis = [series.values[off_s]] + forecasts[i]\n",
    "        pyplot.plot(xaxis, yaxis, color='red')\n",
    "    # show the plot\n",
    "    pyplot.show()\n",
    "\n",
    "# load dataset\n",
    "\n",
    "\n",
    "def parser(x):\n",
    "    return datetime.strptime(x, '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T02:48:25.279652Z",
     "start_time": "2020-04-25T02:48:25.265414Z"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Maybe you called ray.init twice by accident? This error can be suppressed by passing in 'ignore_reinit_error=True' or by calling 'ray.shutdown()' prior to 'ray.init()'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-b4fc433a1dd8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnum_cpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpsutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogical\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_cpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_cpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/ray/worker.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(address, redis_address, redis_port, num_cpus, num_gpus, memory, object_store_memory, resources, driver_object_store_memory, redis_max_memory, log_to_driver, node_ip_address, object_id_seed, local_mode, redirect_worker_output, redirect_output, ignore_reinit_error, num_redis_shards, redis_max_clients, redis_password, plasma_directory, huge_pages, include_java, include_webui, webui_host, job_id, configure_logging, logging_level, logging_format, plasma_store_socket_name, raylet_socket_name, temp_dir, load_code_from_local, use_pickle, _internal_config, lru_evict)\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m             raise RuntimeError(\"Maybe you called ray.init twice by accident? \"\n\u001b[0m\u001b[1;32m    687\u001b[0m                                \u001b[0;34m\"This error can be suppressed by passing in \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                                \u001b[0;34m\"'ignore_reinit_error=True' or by calling \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Maybe you called ray.init twice by accident? This error can be suppressed by passing in 'ignore_reinit_error=True' or by calling 'ray.shutdown()' prior to 'ray.init()'."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import psutil\n",
    "import ray\n",
    "import scipy.signal\n",
    "\n",
    "num_cpus = psutil.cpu_count(logical=False)\n",
    "\n",
    "ray.init(num_cpus=num_cpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T02:48:32.350591Z",
     "start_time": "2020-04-25T02:48:31.891082Z"
    }
   },
   "outputs": [],
   "source": [
    "data = read_csv('us-counties.csv', header=0, index_col=0, squeeze = True, parse_dates=[0], usecols=[0,3,5], date_parser=parser)\n",
    "unique_fips = data.fips.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T02:48:32.916175Z",
     "start_time": "2020-04-25T02:48:32.908333Z"
    }
   },
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def train(fips):\n",
    "    print(\"Current Fips: \" + str(fips))\n",
    "    series = data[data[\"fips\"] == fips].drop([\"fips\"], axis=1)\n",
    "    # configure\n",
    "\n",
    "    # Given 17 previous days, predit the next 14\n",
    "    n_lag = 17\n",
    "    n_seq = 14\n",
    "\n",
    "    # Two predictions\n",
    "    n_test = 2\n",
    "\n",
    "    n_epochs = 1000\n",
    "    n_batch = 1\n",
    "    n_neurons = 1\n",
    "    # prepare data\n",
    "    scaler, train, test = prepare_data(series, n_test, n_lag, n_seq)\n",
    "    # fit model\n",
    "    model = fit_lstm(train, n_lag, n_seq, n_batch, n_epochs, n_neurons)\n",
    "    # make forecasts\n",
    "    forecasts = make_forecasts(model, n_batch, train, test, n_lag, n_seq)\n",
    "    # inverse transform forecasts and test\n",
    "    forecasts = inverse_transform(series, forecasts, scaler, n_test+2)\n",
    "    actual = [row[n_lag:] for row in test]\n",
    "    actual = inverse_transform(series, actual, scaler, n_test+2)\n",
    "    # evaluate forecasts\n",
    "    evaluate_forecasts(actual, forecasts, n_lag, n_seq)\n",
    "    \n",
    "    return [series, forecasts, actual, ntest+2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T02:50:13.193450Z",
     "start_time": "2020-04-25T02:50:13.154321Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d534e1fdf65458cb841773c999670c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RayTaskError(AttributeError)",
     "evalue": "\u001b[36mray::__main__.train()\u001b[39m (pid=17270, ip=131.215.155.73)\n  File \"python/ray/_raylet.pyx\", line 452, in ray._raylet.execute_task\n  File \"<ipython-input-30-8e5405a3be82>\", line 20, in train\n  File \"<ipython-input-27-d636004017e4>\", line 93, in fit_lstm\nAttributeError: module 'tensorflow' has no attribute 'get_default_graph'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRayTaskError(AttributeError)\u001b[0m              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-dced9baeaee8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# get the jobs back\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/ray/worker.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(object_ids, timeout)\u001b[0m\n\u001b[1;32m   1511\u001b[0m                     \u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore_worker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump_object_store_memory_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1512\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRayTaskError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1513\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_instanceof_cause\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1514\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1515\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRayTaskError(AttributeError)\u001b[0m: \u001b[36mray::__main__.train()\u001b[39m (pid=17270, ip=131.215.155.73)\n  File \"python/ray/_raylet.pyx\", line 452, in ray._raylet.execute_task\n  File \"<ipython-input-30-8e5405a3be82>\", line 20, in train\n  File \"<ipython-input-27-d636004017e4>\", line 93, in fit_lstm\nAttributeError: module 'tensorflow' has no attribute 'get_default_graph'"
     ]
    }
   ],
   "source": [
    "# send out the jobs\n",
    "result_ids = []\n",
    "fip_order = []\n",
    "for fips in tqdm(unique_fips[:3]):\n",
    "    result_ids.append(train.remote(fips))\n",
    "    fip_order.append(fips)\n",
    "\n",
    "# get the jobs back\n",
    "results = ray.get(result_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
